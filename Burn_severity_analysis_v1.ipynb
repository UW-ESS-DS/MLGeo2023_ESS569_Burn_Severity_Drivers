{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps \n",
    "1. Characterize the data \n",
    "2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import files as fl\n",
    "import numpy as np\n",
    "import wget as wget\n",
    "import matplotlib.pyplot as plt \n",
    "# import geopandas as gpd\n",
    "# import rasterio as rs \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1825\n",
      "Number of columns: 29\n",
      "Column Names and Index Positions:\n",
      "Column 0: Unnamed: 0\n",
      "Column 1: RBR\n",
      "Column 2: X\n",
      "Column 3: Y\n",
      "Column 4: AET_1981_2010\n",
      "Column 5: dist_streams_wetlands\n",
      "Column 6: ESI\n",
      "Column 7: FRS\n",
      "Column 8: T_max_bpd\n",
      "Column 9: pct_canopy_cover\n",
      "Column 10: canopy_rumple\n",
      "Column 11: pctl_25_canopy_height\n",
      "Column 12: pctl_95_canopy_height\n",
      "Column 13: Pct_1_tree_clumps\n",
      "Column 14: Pct_2_4_tree_clumps\n",
      "Column 15: Pct_5_9_tree_clumps\n",
      "Column 16: Pct_gteq10_tree_clumps\n",
      "Column 17: topo_aspect\n",
      "Column 18: topo_curvature_45\n",
      "Column 19: topo_plan_curvature\n",
      "Column 20: topo_slope_135\n",
      "Column 21: topo_SRI\n",
      "Column 22: topo_SRI_270\n",
      "Column 23: topo_TPI_4000\n",
      "Column 24: topo_TPI_500\n",
      "Column 25: SCF\n",
      "Column 26: wind_northsouthness\n",
      "Column 27: Rx\n",
      "Column 28: EVT_PP\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset \n",
    "df_path = \"df_for_analysis_ESS569.csv\"\n",
    "df_orig = pd.read_csv(df_path)\n",
    "\n",
    "# Check data frame shape\n",
    "df_shape = df_orig.shape\n",
    "\n",
    "# Extract the number of rows and columns\n",
    "num_rows, num_columns = df_shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "\n",
    "# Number of rows: 1825\n",
    "# Number of columns: 29\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean-up the dataset \n",
    "# Print column names \n",
    "column_names = df_orig.columns\n",
    "\n",
    "# Print the column names along with their index positions\n",
    "print(\"Column Names and Index Positions:\")\n",
    "for i, column_name in enumerate(column_names):\n",
    "    print(f\"Column {i}: {column_name}\")\n",
    "\n",
    "# Remove coordinates and unwnated columns from df_orig \n",
    "df = df_orig.drop(columns=['X', 'Y'])\n",
    "\n",
    "# Check NAs\n",
    "columns_with_nans = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "if columns_with_nans:\n",
    "    print(\"Columns with NA values:\", columns_with_nans)\n",
    "else:\n",
    "    print(\"No NA values in any column.\")\n",
    "\n",
    "# No NA values in any column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests regeression model \n",
    "\n",
    "# Define predictor features (X) and target variable (y)\n",
    "X = df.drop(columns=['RBR'])  # 'RBR' is the target variable\n",
    "y = df['RBR']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Hyperparameter Tuning (Number of Trees, Maximum Depth, Minimum Samples per Leaf, and Number of Predictors)\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 500],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30, 40],  # Maximum depth of each tree\n",
    "    'min_samples_leaf': [5, 10 ,15 ,20],  # Minimum samples per leaf node\n",
    "    'max_features': ['auto', 'sqrt', 'log2']  # Number of predictors to consider at each split\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=2) # n_jobs specifies the number of CPU cores to use for parallel computation \n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Save the GridSearchCV object to a file\n",
    "dump(grid_search, 'grid_search_model.joblib')\n",
    "\n",
    "# Load the GridSearchCV object from the file\n",
    "loaded_grid_search = load('grid_search_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Cross-Validation\n",
    "best_rf = grid_search.best_estimator_\n",
    "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=10, scoring='r2')\n",
    "avg_cv_r2 = np.mean(cv_scores)\n",
    "\n",
    "# Step 3: Print Best Hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fit the Random Forest Regression Model\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the Model (R-squared and RMSE) on Train and Test Data\n",
    "y_train_pred = best_rf.predict(X_train)\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(f\"R-squared (Train): {r2_train:.4f}\")\n",
    "print(f\"RMSE (Train): {rmse_train:.4f}\")\n",
    "print(f\"R-squared (Test): {r2_test:.4f}\")\n",
    "print(f\"RMSE (Test): {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Plot Variable Importances based on Permutation\n",
    "importances = best_rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in best_rf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Variable Importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"b\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance using mean_squared_error(train, pred), and r2_score(train, and pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
